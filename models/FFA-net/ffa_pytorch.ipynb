{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ffa_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJAHn14e+rG2YJqDV+u2wN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abubakrsiddq/BinaryMap/blob/master/models/FFA-net/ffa_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ4HWHqg3NWE"
      },
      "source": [
        "import os, sys\n",
        "import time, math\n",
        "import argparse, random\n",
        "from math import exp\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as tfs\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import functional as FF\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFFqG0px3W8G",
        "outputId": "1ddcb238-cf0e-494c-c0b3-d6e0a07893b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA-Ckfqi3PwB",
        "outputId": "dbf3b17a-1b3a-476c-8309-65692834af90"
      },
      "source": [
        "steps = 2#0000\n",
        "# Device name\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# resume Training\n",
        "resume = False\n",
        "# number of evaluation steps\n",
        "eval_step = 2#5000\n",
        "# learning rate\n",
        "learning_rate = 0.0001\n",
        "# pre-trained model directory\n",
        "pretrained_model_dir = '/content/drive/MyDrive/'#'../input/ffa-net-for-single-image-dehazing-pytorch/trained_models/'\n",
        "# directory to save models to\n",
        "model_dir = '.'\n",
        "# train data\n",
        "trainset = 'its_train'\n",
        "# test data\n",
        "testset = 'its_test'\n",
        "# model to be used\n",
        "network = 'ffa'\n",
        "# residual_groups\n",
        "gps = 3\n",
        "# residual_blocks\n",
        "blocks = 19\n",
        "# batch size\n",
        "bs = 1\n",
        "# crop image\n",
        "crop = True\n",
        "# Takes effect when crop = True\n",
        "crop_size = 50\n",
        "# No lr cos schedule\n",
        "no_lr_sche = True\n",
        "# perceptual loss\n",
        "perloss = False\n",
        "\n",
        "model_name = trainset + '_' + network.split('.')[0] + '_' + str(gps) + '_' + str(blocks)\n",
        "pretrained_model_dir = pretrained_model_dir + model_name + '.pk'\n",
        "model_dir = model_dir + model_name + '.pk'\n",
        "log_dir = 'logs/' + model_name\n",
        "\n",
        "if not os.path.exists('trained_models'):\n",
        "    print(\"error\")\n",
        "    os.mkdir('trained_models')\n",
        "if not os.path.exists('numpy_files'):\n",
        "    os.mkdir('numpy_files')\n",
        "if not os.path.exists('logs'):\n",
        "    os.mkdir('logs')\n",
        "if not os.path.exists('samples'):\n",
        "    os.mkdir('samples')\n",
        "if not os.path.exists(f\"samples/{model_name}\"):\n",
        "    os.mkdir(f'samples/{model_name}')\n",
        "if not os.path.exists(log_dir):\n",
        "    os.mkdir(log_dir)\n",
        "    \n",
        "crop_size='whole_img'\n",
        "if crop:\n",
        "    crop_size = crop_size"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmIOH38Y3d9l"
      },
      "source": [
        "def tensorShow(tensors,titles=None):\n",
        "    '''t:BCWH'''\n",
        "    fig=plt.figure()\n",
        "    for tensor, title, i in zip(tensors, titles, range(len(tensors))):\n",
        "        img = make_grid(tensor)\n",
        "        npimg = img.numpy()\n",
        "        ax = fig.add_subplot(211+i)\n",
        "        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        ax.set_title(title)\n",
        "    plt.show()\n",
        "    \n",
        "def lr_schedule_cosdecay(t, T, init_lr=learning_rate):\n",
        "    lr=0.5*(1+math.cos(t*math.pi/T))*init_lr\n",
        "    return lr"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QcarwFf3h0P"
      },
      "source": [
        "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
        "    \n",
        "    \n",
        "class PALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(PALayer, self).__init__()\n",
        "        self.pa = nn.Sequential(\n",
        "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.pa(x)\n",
        "        return x * y\n",
        "\n",
        "    \n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(CALayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ca = nn.Sequential(\n",
        "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.ca(y)\n",
        "        return x * y\n",
        "\n",
        "    \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size,):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.act1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n",
        "        self.calayer = CALayer(dim)\n",
        "        self.palayer = PALayer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.act1(self.conv1(x))\n",
        "        res = res+x \n",
        "        res = self.conv2(res)\n",
        "        res = self.calayer(res)\n",
        "        res = self.palayer(res)\n",
        "        res += x \n",
        "        return res\n",
        "\n",
        "    \n",
        "class Group(nn.Module):\n",
        "    def __init__(self, conv, dim, kernel_size, blocks):\n",
        "        super(Group, self).__init__()\n",
        "        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n",
        "        modules.append(conv(dim, dim, kernel_size))\n",
        "        self.gp = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.gp(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "    \n",
        "class FFA(nn.Module):\n",
        "    def __init__(self,gps,blocks,conv=default_conv):\n",
        "        super(FFA, self).__init__()\n",
        "        self.gps = gps\n",
        "        self.dim = 64\n",
        "        kernel_size = 3\n",
        "        pre_process = [conv(3, self.dim, kernel_size)]\n",
        "        assert self.gps==3\n",
        "        self.g1 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.g2 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.g3 = Group(conv, self.dim, kernel_size,blocks=blocks)\n",
        "        self.ca = nn.Sequential(*[\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(self.dim*self.gps,self.dim//16,1,padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.dim//16, self.dim*self.gps, 1, padding=0, bias=True),\n",
        "            nn.Sigmoid()\n",
        "            ])\n",
        "        self.palayer = PALayer(self.dim)\n",
        "\n",
        "        post_process = [\n",
        "            conv(self.dim, self.dim, kernel_size),\n",
        "            conv(self.dim, 3, kernel_size)]\n",
        "\n",
        "        self.pre = nn.Sequential(*pre_process)\n",
        "        self.post = nn.Sequential(*post_process)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x = self.pre(x1)\n",
        "        res1 = self.g1(x)\n",
        "        res2 = self.g2(res1)\n",
        "        res3 = self.g3(res2)\n",
        "        w = self.ca(torch.cat([res1,res2,res3],dim=1))\n",
        "        w = w.view(-1,self.gps, self.dim)[:,:,:,None,None]\n",
        "        out = w[:,0,::] * res1 + w[:,1,::] * res2+w[:,2,::] * res3\n",
        "        out = self.palayer(out)\n",
        "        x = self.post(out)\n",
        "        return x + x1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf6gxByQ3kMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "c5971e24-aaaa-49b1-ee3c-65125a15198b"
      },
      "source": [
        "# --- Perceptual loss network  --- #\n",
        "class PerLoss(torch.nn.Module):\n",
        "    def __init__(self, vgg_model):\n",
        "        super(PerLoss, self).__init__()\n",
        "        self.vgg_layers = vgg_model\n",
        "        self.layer_name_mapping = {\n",
        "            '3': \"relu1_2\",\n",
        "            '8': \"relu2_2\",\n",
        "            '15': \"relu3_3\"\n",
        "        }\n",
        "\n",
        "    def output_features(self, x):\n",
        "        output = {}\n",
        "        for name, module in self.vgg_layers._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.layer_name_mapping:\n",
        "                output[self.layer_name_mapping[name]] = x\n",
        "        return list(output.values())\n",
        "\n",
        "    def forward(self, dehaze, gt):\n",
        "        loss = []\n",
        "        dehaze_features = self.output_features(dehaze)\n",
        "        gt_features = self.output_features(gt)\n",
        "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
        "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
        "\n",
        "        return sum(loss)/len(loss)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cf10dab02474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Perceptual loss network  --- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPerLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPerLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un_1DybG3myP"
      },
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "    img1=torch.clamp(img1,min=0,max=1)\n",
        "    img2=torch.clamp(img2,min=0,max=1)\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
        "\n",
        "def psnr(pred, gt):\n",
        "    pred=pred.clamp(0,1).cpu().numpy()\n",
        "    gt=gt.clamp(0,1).cpu().numpy()\n",
        "    imdff = pred - gt\n",
        "    rmse = math.sqrt(np.mean(imdff ** 2))\n",
        "    if rmse == 0:\n",
        "        return 100\n",
        "    return 20 * math.log10( 1.0 / rmse)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_A-kgB3sHo"
      },
      "source": [
        "class RESIDE_Dataset(data.Dataset):\n",
        "    def __init__(self, path, train, size=crop_size, format='.png'):\n",
        "        super(RESIDE_Dataset, self).__init__()\n",
        "        self.size = size\n",
        "        self.train = train\n",
        "        self.format = format\n",
        "        self.haze_imgs_dir = os.listdir(os.path.join(path,'hazy'))\n",
        "        self.haze_imgs = [os.path.join(path, 'hazy', img) for img in self.haze_imgs_dir]\n",
        "        self.haze_imgs=self.haze_imgs[:500]\n",
        "        self.clear_dir = os.path.join(path,'gt')\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        haze = Image.open(self.haze_imgs[index])\n",
        "        if isinstance(self.size, int):\n",
        "            while haze.size[0] < self.size or haze.size[1] < self.size :\n",
        "                index = random.randint(0, 20000)\n",
        "                haze = Image.open(self.haze_imgs[index])\n",
        "        img = self.haze_imgs[index]\n",
        "        id = img.split('/')[-1].split('_')[0]\n",
        "        clear_name = id + self.format\n",
        "        clear = Image.open(os.path.join(self.clear_dir, clear_name))\n",
        "        clear = tfs.CenterCrop(haze.size[::-1])(clear)\n",
        "        if not isinstance(self.size, str):\n",
        "            i, j, h, w = tfs.RandomCrop.get_params(haze, output_size=(self.size, self.size))\n",
        "            haze = FF.crop(haze, i, j, h, w)\n",
        "            clear = FF.crop(clear, i, j, h, w)\n",
        "        haze, clear = self.augData(haze.convert(\"RGB\"), clear.convert(\"RGB\") )\n",
        "        return haze, clear\n",
        "    \n",
        "    def augData(self, data, target):\n",
        "        if self.train:\n",
        "            rand_hor = random.randint(0,1)\n",
        "            rand_rot = random.randint(0,3)\n",
        "            data = tfs.RandomHorizontalFlip(rand_hor)(data)\n",
        "            target = tfs.RandomHorizontalFlip(rand_hor)(target)\n",
        "            if rand_rot:\n",
        "                data = FF.rotate(data, 90*rand_rot)\n",
        "                target = FF.rotate(target, 90*rand_rot)\n",
        "        data = tfs.ToTensor()(data)\n",
        "        data = tfs.Normalize(mean=[0.64,0.6,0.58], std=[0.14,0.15,0.152])(data)\n",
        "        target = tfs.ToTensor()(target)\n",
        "        return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.haze_imgs)\n",
        "\n",
        "\n",
        "# path to your 'data' folder\n",
        "its_train_path = '/content/drive/MyDrive/kitti/'\n",
        "its_test_path = '/content/drive/MyDrive/test_set/'\n",
        "\n",
        "ITS_train_loader = DataLoader(dataset=RESIDE_Dataset(its_train_path, train=True, size=crop_size), batch_size=bs, shuffle=True)\n",
        "ITS_test_loader = DataLoader(dataset=RESIDE_Dataset(its_test_path, train=False, size='whole img'), batch_size=1, shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSw7LjO3uQ4J"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asdw4G8f3s0n",
        "outputId": "dfd92dc8-f269-49da-bac6-30bdd5cb7ca6"
      },
      "source": [
        "print('log_dir :', log_dir)\n",
        "print('model_name:', model_name)\n",
        "\n",
        "models_ = {'ffa': FFA(gps = gps, blocks = blocks)}\n",
        "loaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader}\n",
        "#loaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader, 'ots_train': OTS_train_loader, 'ots_test': OTS_test_loader}\n",
        "start_time = time.time()\n",
        "T = steps\n",
        "\n",
        "def train(net, loader_train, loader_test, optim, criterion):\n",
        "    losses = []\n",
        "    start_step = 0\n",
        "    #max_ssim = max_psnr = 0\n",
        "    ssims, psnrs = [], []\n",
        "    if resume and os.path.exists(pretrained_model_dir):\n",
        "        print(f'resume from {pretrained_model_dir}')\n",
        "        ckp = torch.load(pretrained_model_dir)\n",
        "        #losses = ckp['losses']\n",
        "        #net.load_state_dict(ckp['model'])\n",
        "        #start_step = ckp['step']\n",
        "        #max_ssim = ckp['max_ssim']\n",
        "        #max_psnr = ckp['max_psnr']\n",
        "        #psnrs = ckp['psnrs']\n",
        "        #ssims = ckp['ssims']\n",
        "        print(f'Resuming training from step: {start_step} ***')\n",
        "    else :\n",
        "        print('Training from scratch *** ')\n",
        "    for step in range(start_step+1, steps+1):\n",
        "        net.train()\n",
        "        lr = learning_rate\n",
        "        if  no_lr_sche:\n",
        "            lr = lr_schedule_cosdecay(step,T)\n",
        "            for param_group in optim.param_groups:\n",
        "                param_group[\"lr\"] = lr\n",
        "        x, y = next(iter(loader_train))\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        out = net(x)\n",
        "        loss = criterion[0](out,y)\n",
        "        if perloss:\n",
        "            loss2 = criterion[1](out,y)\n",
        "            loss = loss + 0.04*loss2\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "        print(f'\\rtrain loss: {loss.item():.5f} | step: {step}/{steps} | lr: {lr :.7f} | time_used: {(time.time()-start_time)/60 :.1f}',end='',flush=True)\n",
        "\n",
        "        if step % eval_step ==0 :\n",
        "            with torch.no_grad():\n",
        "                ssim_eval, psnr_eval = test(net, loader_test, max_psnr, max_ssim, step)\n",
        "            print(f'\\nstep: {step} ')\n",
        "\n",
        "            #ssims.append(ssim_eval)\n",
        "            #psnrs.append(psnr_eval)\n",
        "            '''if ssim_eval > max_ssim and psnr_eval > max_psnr :\n",
        "                max_ssim = max(max_ssim,ssim_eval)\n",
        "                max_psnr = max(max_psnr,psnr_eval)\n",
        "                torch.save({\n",
        "                            'step': step,\n",
        "                            'max_psnr': max_psnr,\n",
        "                            'max_ssim': max_ssim,\n",
        "                            'ssims': ssims,\n",
        "                            'psnrs': psnrs,\n",
        "                            'losses': losses,\n",
        "                            'model': net.state_dict()\n",
        "                }, model_dir)'''\n",
        "                #print(f'\\n model saved at step : {step} | max_psnr: {max_psnr:.4f} | max_ssim: {max_ssim:.4f}')\n",
        "\n",
        "    #np.save(f'./numpy_files/{model_name}_{steps}_losses.npy',losses)\n",
        "    #np.save(f'./numpy_files/{model_name}_{steps}_ssims.npy',ssims)\n",
        "    #np.save(f'./numpy_files/{model_name}_{steps}_psnrs.npy',psnrs)\n",
        "\n",
        "def test(net, loader_test, max_psnr, max_ssim, step):\n",
        "    net.eval()\n",
        "    torch.cuda.empty_cache()\n",
        "    ssims, psnrs = [], []\n",
        "    for i, (inputs, targets) in enumerate(loader_test):\n",
        "        inputs = inputs.to(device); targets = targets.to(device)\n",
        "        pred = net(inputs)\n",
        "        # # print(pred)\n",
        "        # tfs.ToPILImage()(torch.squeeze(targets.cpu())).save('111.png')\n",
        "        # vutils.save_image(targets.cpu(),'target.png')\n",
        "        # vutils.save_image(pred.cpu(),'pred.png')\n",
        "        ssim1 = ssim(pred, targets).item()\n",
        "        psnr1 = psnr(pred, targets)\n",
        "        ssims.append(ssim1)\n",
        "        psnrs.append(psnr1)\n",
        "        #if (psnr1>max_psnr or ssim1 > max_ssim) and s :\n",
        "#             ts=vutils.make_grid([torch.squeeze(inputs.cpu()),torch.squeeze(targets.cpu()),torch.squeeze(pred.clamp(0,1).cpu())])\n",
        "#             vutils.save_image(ts,f'samples/{model_name}/{step}_{psnr1:.4}_{ssim1:.4}.png')\n",
        "#             s=False\n",
        "    return np.mean(ssims) ,np.mean(psnrs)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir : logs/its_train_ffa_3_19\n",
            "model_name: its_train_ffa_3_19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJOk3HkWjGBv"
      },
      "source": [
        "pretrained_model_dir='/content/drive/MyDrive/ots_train_ffa_3_19.pk'\n",
        "ckp = torch.load(pretrained_model_dir, map_location='cpu')\n",
        "weights={}\n",
        "for k,v in ckp['model'].items():\n",
        "  weights[k[7:]]=v"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3v-Whfv30rw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "929f6328-b84c-4a36-daf0-572388c03190"
      },
      "source": [
        "%%time\n",
        "device='cuda'\n",
        "loader_train = loaders_[trainset]\n",
        "loader_test = loaders_[testset]\n",
        "net = FFA(gps=3,blocks=10)#models_[network]\n",
        "#net.load_state_dict(state_dict=weights)\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "#net.load_state_dict(ckp['model'])\n",
        "criterion = []\n",
        "criterion.append(nn.L1Loss().to('cuda'))\n",
        "if perloss:\n",
        "    vgg_model = vgg16(pretrained=True).features[:16]\n",
        "    vgg_model = vgg_model.to(device)\n",
        "    for param in vgg_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    criterion.append(PerLoss(vgg_model).to(device))\n",
        "optimizer = optim.Adam(params = filter(lambda x: x.requires_grad, net.parameters()), lr=learning_rate, betas=(0.9,0.999), eps=1e-08)\n",
        "optimizer.zero_grad()\n",
        "train(net, loader_train, loader_test, optimizer, criterion)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training from scratch *** \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-24b261e3d709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"device='cuda'\\nloader_train = loaders_[trainset]\\nloader_test = loaders_[testset]\\nnet = FFA(gps=3,blocks=10)#models_[network]\\n#net.load_state_dict(state_dict=weights)\\nnet = net.to(device)\\nif device == 'cuda':\\n    net = torch.nn.DataParallel(net)\\n    cudnn.benchmark = True\\n\\n\\n#net.load_state_dict(ckp['model'])\\ncriterion = []\\ncriterion.append(nn.L1Loss().to('cuda'))\\nif perloss:\\n    vgg_model = vgg16(pretrained=True).features[:16]\\n    vgg_model = vgg_model.to(device)\\n    for param in vgg_model.parameters():\\n        param.requires_grad = False\\n    criterion.append(PerLoss(vgg_model).to(device))\\noptimizer = optim.Adam(params = filter(lambda x: x.requires_grad, net.parameters()), lr=learning_rate, betas=(0.9,0.999), eps=1e-08)\\noptimizer.zero_grad()\\ntrain(net, loader_train, loader_test, optimizer, criterion)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4e5c5c77254d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, loader_train, loader_test, optim, criterion)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mperloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-23cef36b176b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mres3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-23cef36b176b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-23cef36b176b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-23cef36b176b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 15.90 GiB total capacity; 14.81 GiB already allocated; 53.75 MiB free; 14.97 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irRIjzlK34O_"
      },
      "source": [
        "# its or ots\n",
        "task = 'its'\n",
        "# test imgs folder\n",
        "test_imgs = '/hazy/'\n",
        "\n",
        "dataset = task\n",
        "img_dir = test_imgs\n",
        "\n",
        "output_dir = f'pred_FFA_{dataset}/'\n",
        "print(\"pred_dir:\",output_dir)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "\n",
        "ckp = torch.load(model_dir, map_location=device)\n",
        "net = FFA(gps=gps, blocks=blocks)\n",
        "net = nn.DataParallel(net)\n",
        "net.load_state_dict(ckp['model'])\n",
        "net.eval()\n",
        "\n",
        "for im in os.listdir(img_dir):\n",
        "    haze = Image.open(img_dir+im)\n",
        "    haze1 = tfs.Compose([\n",
        "        tfs.ToTensor(),\n",
        "        tfs.Normalize(mean=[0.64, 0.6, 0.58],std=[0.14,0.15, 0.152])\n",
        "    ])(haze)[None,::]\n",
        "    haze_no = tfs.ToTensor()(haze)[None,::]\n",
        "    with torch.no_grad():\n",
        "        pred = net(haze1)\n",
        "    ts = torch.squeeze(pred.clamp(0,1).cpu())\n",
        "    # tensorShow([haze_no, pred.clamp(0,1).cpu()],['haze', 'pred'])\n",
        "    \n",
        "    haze_no = make_grid(haze_no, nrow=1, normalize=True)\n",
        "    ts = make_grid(ts, nrow=1, normalize=True)\n",
        "    image_grid = torch.cat((haze_no, ts), -1)\n",
        "    vutils.save_image(image_grid, output_dir+im.split('.')[0]+'_FFA.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}